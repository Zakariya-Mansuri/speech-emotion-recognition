In this project, I developed a Convolutional Neural Network (CNN) to recognize emotions from speech signals. I utilized a dataset comprising four different datasets, namely RAVDESS, CREMA-D, TESS, and SAVEE. To extract features for the audio signals, I used Mel-Frequency Cepstral Coefficients (MFCCs). 

The CNN architecture employed Conv1D layers with batch normalization, max pooling, and a dense output layer with softmax activation. The emotions recognized were neutral, happy, sad, angry, fear, disgust, and surprise. 

After training the model, it achieved an accuracy of 96% on the test set and an F1 score of 96%. These results demonstrate the potential of CNNs in recognizing emotions from speech signals. 

This technology has various applications, including human-robot interaction, speech therapy, and mental health diagnosis. For instance, the emotion recognition system can be integrated with robots to improve their ability to interact with humans by enabling them to recognize human emotions and respond appropriately. 

In speech therapy, the technology can be used to monitor the progress of patients and provide personalized feedback based on their emotional responses. The system can also aid in diagnosing mental health disorders by analyzing patterns in the speech of patients and identifying changes in their emotional state. 

In conclusion, this project demonstrates the potential of CNNs in recognizing emotions from speech signals and highlights the usefulness of such technology in various applications. With further development and refinement, this technology has the potential to revolutionize human-robot interaction, speech therapy, and mental health diagnosis.